

# Resources Used 
CMU-MOSEI Dataset | Papers With Code (CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics.)
https://paperswithcode.com/dataset/cmu-mosei

\bibitem{01} Andrea C. Samson, Sylvia D. Kreibig, B. Soderstrom, A. Ayanna Wad, “Eliciting positive, negative and mixed emotional states: A film library for affective scientists.”, Cognition and Emotion, vol. 30, no. 5, 2015, pp. 827–856.
https://doi.org/10.1080/02699931.2015.1031089  

\bibitem{02}  Beatrice de Gelder ,”Why bodies? Twelve reasons for including bodily expressions in affective neuroscience.”, Phil. Trans. R. Soc. B (2009) 364, 3475–3484.
https://doi.org/10.1098/rstb.2009.0190  

\bibitem{03} D. Das, S. Bandyopadhyay : Sentence level emotion tagging. In: 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, pp. 1–6. IEEE (2009).
https://doi.org/10.1109/acii.2009.5349598  

\bibitem{04} Tegar S. Utomo, R. Sarno and Suhariyanto,”Emotion Label from ANEW dataset for Searching Best Definition from WordNet”, International Seminar on Application for Technology of Information and Communication (2018) 249-252.
https://doi.org/10.1109/isemantic.2018.8549769  

\bibitem{SVM} T. K. Landauer and S. T. Dumais, “A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.” Psychological review, vol. 104, no. 2, p. 211, 1997.
https://doi.org/10.1037/0033-295x.104.2.211  

\bibitem{lit2} Eissa M.Alshari and A. Azman, ”Effective Method for Sentiment Lexical Dictionary Enrichment based on Word2Vec for Sentiment Analysis”, Fourth Int. Conf. on Information Retrieval and Knowledge Management, 2018
https://doi.org/10.1109/infrkm.2018.8464775  

\bibitem{stat} Asghar, Muhammad Zubair, et al. “Sentence-Level Emotion Detection Framework Using Rule-Based Classification.” Cognitive Computation, vol. 9, no. 6, 2017, pp. 868–894., doi:10.1007/s12559-017-9503-3.
https://doi.org/10.1007/s12559-017-9503-3  

\bibitem{sa} 
Samson, Andrea C., et al. “Eliciting Positive, Negative and Mixed Emotional States: A Film Library for Affective Scientists.” Cognition and Emotion, vol. 30, no. 5, 2015, pp. 827–856., doi:10.1080/02699931.2015.1031089. Strapparava C., Mihalcea R.: SemEval-2007 task 14: affective text. In: Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pp. 70–74 (2007)
https://doi.org/10.3115/1621474.1621487  

\bibitem{sa2} [used] R. Dong, O. Peng, X. Li and X. Guan, ”CNN-SVM with Embedded Recurrent Structure for Social Emotion Prediction,” 2018 Chinese Au- tomation Congress (CAC), 2018, pp. 3024-3029
https://doi.org/10.1109/cac.2018.8623318  

\bibitem{sa3} 
Strapparava, C., Mihalcea, R.: Learning to identify emotions in text. In:Proceedings of the 2008 ACM Symposium on Applied Computing, pp. 1556–1560 (2008).
https://doi.org/10.1145/1363686.1364052  

\bibitem{sa4} Shaheen, S., El-Hajj, W., Hajj, H., Elbassuoni, S.: Emotion recognition from text based on automatically generated rules. In: IEEE International Conference on Data Mining Workshop, pp. 383–392 (2014)
https://doi.org/10.1109/icdmw.2014.80  

\bibitem{sa5} M. S. ̈Ozerdem and H. Polat, “Emotion recognition based on EEG features in movie clips with channel selection,” Brain Inf., vol. 4, no. 4, pp. 241–252, 2017.
https://doi.org/10.1007/s40708-017-0069-3  

\bibitem{sa6}  Hongli Zhang , Alireza Jolfaei , and Mamoun Alazab, “A Face Emotion Recognition Method Using Convolutional Neural Network and Image Edge Computing,” IEEE ACCESS, Nov, 2019. 2949741
https://doi.org/10.1109/access.2019.2949741  

\bibitem{sa8} H. Ma and T. Celik, “FER-Net: Facial expression recognition using densely connected convolutional network,” Electron. Lett., vol. 55, no. 4, pp. 184–186, Feb. 2019.
https://doi.org/10.1049/el.2018.7871  

\bibitem{sa10} A. V. Savchenko, “Deep neural networks and maximum likelihood search for approximate nearest neighbor in video-based image recognition,” Opt. Memory Neural Netw., vol. 26, no. 2, pp. 129–136, Apr. 2017.
https://doi.org/10.3103/s1060992x17020102  

\bibitem{sa11} Salah, Albert Ali. Multimodal Behavior Analysis in the Wild Video-based emotion recognition in the wild. , (2019), 369-386
https://doi.org/10.1016/b978-0-12-814601-9.00031-6  

