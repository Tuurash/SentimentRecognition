<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link href="Crossref%20Metadata%20Search_files/bootstrap.css" media="screen" rel="stylesheet" type="text/css">
<link href="Crossref%20Metadata%20Search_files/font-awesome.css" media="screen" rel="stylesheet" type="text/css">
<link href="Crossref%20Metadata%20Search_files/font.css" rel="stylesheet" type="text/css">
<link href="Crossref%20Metadata%20Search_files/typeahead.css" media="screen" rel="stylesheet" type="text/css">
<link href="https://search.crossref.org/opensearch.xml" rel="search" title="Crossref Metadata Search" type="application/opensearchdescription+xml">
<script type="text/javascript" async="" defer="defer" src="Crossref%20Metadata%20Search_files/matomo.js"></script><script src="Crossref%20Metadata%20Search_files/jquery_002.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/bootstrap_002.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/spin.js" type="text/javascript"></script><style></style>
<script src="Crossref%20Metadata%20Search_files/format.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/typeahead.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/jquery.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/bootstrap.js" type="text/javascript"></script>
<script src="Crossref%20Metadata%20Search_files/hogan.js" type="text/javascript"></script>
<title>Crossref Metadata Search</title>
<link rel="apple-touch-icon" sizes="57x57" href="https://assets.crossref.org/favicon/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="https://assets.crossref.org/favicon/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="https://assets.crossref.org/favicon/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="https://assets.crossref.org/favicon/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="https://assets.crossref.org/favicon/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="https://assets.crossref.org/favicon/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="https://assets.crossref.org/favicon/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="https://assets.crossref.org/favicon/apple-touch-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://assets.crossref.org/favicon/apple-touch-icon-180x180.png">
<link rel="icon" type="image/png" href="https://assets.crossref.org/favicon/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="https://assets.crossref.org/favicon/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="https://assets.crossref.org/favicon/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="https://assets.crossref.org/favicon/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="https://assets.crossref.org/favicon/manifest.json">
<link rel="mask-icon" href="https://assets.crossref.org/favicon/safari-pinned-tab.svg" color="#5bbad5">
<meta name="apple-mobile-web-app-title" content="Crossref">
<meta name="application-name" content="Crossref">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-TileImage" content="https://assets.crossref.org/favicon/mstile-144x144.png">
<meta name="theme-color" content="#ffffff">
<script>
   var _paq = window._paq || [];
   /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
   _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
   _paq.push(["setCookieDomain", "*.search.crossref.org"]);
   _paq.push(['trackPageView']);
   _paq.push(['enableLinkTracking']);
   (function() {
   var u="https://crossref.matomo.cloud/";
   _paq.push(['setTrackerUrl', u+'matomo.php']);
   _paq.push(['setSiteId', '17']);
   var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
   g.type='text/javascript'; g.async=true; g.defer=true; g.src='//cdn.matomo.cloud/crossref.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
   })();
</script>
<noscript><p><img src="https://crossref.matomo.cloud/matomo.php?idsite=17&rec=1" style="border:0;" alt="" /></p></noscript>
<style>
  a, a:visited {
    color: rgb(56, 166, 203);
  }
  a:hover {
    color: rgb(6, 116, 153);
  }
</style>
<script>
  $(document).ready(function() {
    $('.number').formatNumber();
  });
</script>
</head>
<body data-new-gr-c-s-check-loaded="8.896.0" data-gr-ext-installed="">
<div class="notifications top-left"></div>
<div class="container-fluid">
<style>
  .cr-red {
    color: rgb(192, 70, 62);
  }
  .home-link, .home-link:visited, .home-link:hover {
    text-decoration: none;
    color: black;
  }
  .simple-nav {
    font-size: 1.2em;
    line-height: 1.1em;
    margin-top: 1em;
  }
  .simple-nav a, .simple-nav a:visited {
    color: grey;
    padding-left: .4em;
  }
  .search-in-header {
    position: relative;
    margin-top: .5em;
    margin-right: 16em;
    width: 22em;
  }
  .search-in-header i {
    position: absolute;
    top: 0.35em;
    left: 0.5em;
    font-size: 1.8em;
    color: #aaa;
  }
  .search-in-header input {
    line-height: 1.2em;
    height: 1.4em;
    font-size: 1.6em;
    width: 22em;
    padding-left: 2em;
  }
  .span-header {
    margin-top: 0.5em;
    margin-bottom: -1em;
  }
  .simple-nav-outer {
    position: relative;
    width: 45em;
  }
  .simple-nav-bump-up {
    margin-top: -1em;
  }
  .simple-nav {
    position: absolute;
    height: 2em;
    top: 50%;
    margin-top: 1em;
    right: 0;
  }
  .simple-nav .pull-right {
    margin-top: -0.05em;
  }
  .simple-nav .pull-right img {
    margin-top: -0.2em;
  }
  .simple-nav > * {
    margin-left: .4em;
  }
  .simple-nav.smaller {
    font-size: 1em;
    line-height: 2em;
  }
</style>
<div class="row-fluid">
<div class="span-header span3" style="padding-top: 0.7em;">
<a class="home-link" href="https://search.crossref.org/">
<img src="Crossref%20Metadata%20Search_files/crossref-logo-landscape-100.png">
</a>
</div>
<div class="span-header span9">
<div class="pull-right">
<div class="simple-nav-outer">
<div class="simple-nav smaller">
<div class="pull-right">
<a class="sign-in-link" href="#">
<img src="Crossref%20Metadata%20Search_files/orcid_24x24.gif">
Sign in
</a>
<script>
  $(document).ready(function() {
    $('.sign-in-link').click(function(e) {
      $.oauthpopup({path: '/auth/orcid',
                    callback: function() {
                      document.location.reload(true);
                    }});
      e.preventDefault();
      return false;
    });
  });
</script>

</div>
<a href="https://search.crossref.org/funding">Funding Data</a>
<a href="https://search.crossref.org/references">Link References</a>
<a href="https://search.crossref.org/help/status">Status</a>
<a href="https://api.crossref.org/">API</a>
<a href="https://search.crossref.org/help/search">Help</a>
</div>
</div>
</div>
<form action="/" id="search-form" method="GET">
<div class="search-in-header">
<input class="input input-xlarge search-input" id="search-input" name="q" placeholder="Title, author, DOI, ORCID iD, etc." type="text">
<i class="icon-search"></i>
<input id="from-ui" name="from_ui" type="hidden" value="yes">
</div>
</form>
</div>
</div>

<div class="row-fluid">
<div class="span2"></div>
<div class="span8">
<div class="page-header">
<h2>Link References</h2>
</div>
<span class="lead">Queried for 22 references</span>
<table class="table table-striped" style="margin-top: 20px">
<tbody><tr>
<td>Reference</td>
</tr>
<tr>
<td>
<br>Andrea C. Samson, Sylvia D. Kreibig, B. Soderstrom, A. Ayanna Wad, 
“Eliciting positive, negative and mixed emotional states: A film library
 for affective scientists.”, Cognition and Emotion, vol. 30, no. 5, 
2015, pp. 827–856.<br>
<a href="https://doi.org/10.1080/02699931.2015.1031089" target="_blank">https://doi.org/10.1080/02699931.2015.1031089</a>
</td>
</tr>
<tr class="error">
<td>P. Ekman : ”An argument for basic emotions. Cognition &amp; Emotjion”, 6(3-4), 169–200 (1992)</td>
<td>Result score too low</td>
</tr>
<tr>
<td>
<br>Beatrice de Gelder ,”Why bodies? Twelve reasons for including bodily
 expressions in affective neuroscience.”, Phil. Trans. R. Soc. B (2009) 
364, 3475–3484.<br>
<a href="https://doi.org/10.1098/rstb.2009.0190" target="_blank">https://doi.org/10.1098/rstb.2009.0190</a>
</td>
</tr>
<tr>
<td>
<br>D. Das, S. Bandyopadhyay : Sentence level emotion tagging. In: 2009 
3rd International Conference on Affective Computing and Intelligent 
Interaction and Workshops, pp. 1–6. IEEE (2009).<br>
<a href="https://doi.org/10.1109/acii.2009.5349598" target="_blank">https://doi.org/10.1109/acii.2009.5349598</a>
</td>
</tr>
<tr>
<td>
<br>Tegar S. Utomo, R. Sarno and Suhariyanto,”Emotion Label from ANEW 
dataset for Searching Best Definition from WordNet”, International 
Seminar on Application for Technology of Information and Communication 
(2018) 249-252.<br>
<a href="https://doi.org/10.1109/isemantic.2018.8549769" target="_blank">https://doi.org/10.1109/isemantic.2018.8549769</a>
</td>
</tr>
<tr>
<td>
<br>T. K. Landauer and S. T. Dumais, “A solution to plato’s problem: The
 latent semantic analysis theory of acquisition, induction, and 
representation of knowledge.” Psychological review, vol. 104, no. 2, p. 
211, 1997.<br>
<a href="https://doi.org/10.1037/0033-295x.104.2.211" target="_blank">https://doi.org/10.1037/0033-295x.104.2.211</a>
</td>
</tr>
<tr>
<td>
<br>Eissa M.Alshari and A. Azman, ”Effective Method for Sentiment 
Lexical Dictionary Enrichment based on Word2Vec for Sentiment Analysis”,
 Fourth Int. Conf. on Information Retrieval and Knowledge Management, 
2018<br>
<a href="https://doi.org/10.1109/infrkm.2018.8464775" target="_blank">https://doi.org/10.1109/infrkm.2018.8464775</a>
</td>
</tr>
<tr class="error">
<td>Vasani, Gaurav B., Prajesh V. Kathiriya, Alpesh J. Thesiya and 
Hardik H. Joshi. “Human Emotional State Recognition Using Facial 
Expression Detection.”, Research Inventy: Int. journal of Eng. and 
Science (2013). Vol-2, issue-2, pp 42-44.</td>
<td>Result score too low</td>
</tr>
<tr>
<td>
<br>Asghar, Muhammad Zubair, et al. “Sentence-Level Emotion Detection 
Framework Using Rule-Based Classification.” Cognitive Computation, vol. 
9, no. 6, 2017, pp. 868–894., doi:10.1007/s12559-017-9503-3.<br>
<a href="https://doi.org/10.1007/s12559-017-9503-3" target="_blank">https://doi.org/10.1007/s12559-017-9503-3</a>
</td>
</tr>
<tr>
<td>
<br>Samson, Andrea C., et al. “Eliciting Positive, Negative and Mixed 
Emotional States: A Film Library for Affective Scientists.” Cognition 
and Emotion, vol. 30, no. 5, 2015, pp. 827–856., 
doi:10.1080/02699931.2015.1031089. Strapparava C., Mihalcea R.: 
SemEval-2007 task 14: affective text. In: Proceedings of the Fourth 
International Workshop on Semantic Evaluations (SemEval-2007), pp. 70–74
 (2007)<br>
<a href="https://doi.org/10.3115/1621474.1621487" target="_blank">https://doi.org/10.3115/1621474.1621487</a>
</td>
</tr>
<tr>
<td>
<br>R. Dong, O. Peng, X. Li and X. Guan, ”CNN-SVM with Embedded 
Recurrent Structure for Social Emotion Prediction,” 2018 Chinese Au- 
tomation Congress (CAC), 2018, pp. 3024-3029<br>
<a href="https://doi.org/10.1109/cac.2018.8623318" target="_blank">https://doi.org/10.1109/cac.2018.8623318</a>
</td>
</tr>
<tr class="error">
<td>International Survey on Emotion Antecedents and Reactions data set. 
https://www. unige.ch/cisa/index.php/download file/view/395/296/ Colin 
Campbell; Yiming Ying, Learning with Support Vector Machines , Morgan 
&amp; Claypool, 2010.</td>
<td>Result score too low</td>
</tr>
<tr>
<td>
<br>Strapparava, C., Mihalcea, R.: Learning to identify emotions in 
text. In:Proceedings of the 2008 ACM Symposium on Applied Computing, pp.
 1556–1560 (2008).<br>
<a href="https://doi.org/10.1145/1363686.1364052" target="_blank">https://doi.org/10.1145/1363686.1364052</a>
</td>
</tr>
<tr class="error">
<td>Francisco, V., Gerv ́as, P.: Exploring the compositionality of 
emotions in text: word emotions, sentence emotions and automated 
tagging. In: AAAI-06 Workshop on Computational Aesthetics: Artificial 
Intelligence Approaches to Beauty and Happiness (2006).</td>
<td>Result score too low</td>
</tr>
<tr>
<td>
<br>Shaheen, S., El-Hajj, W., Hajj, H., Elbassuoni, S.: Emotion 
recognition from text based on automatically generated rules. In: IEEE 
International Conference on Data Mining Workshop, pp. 383–392 (2014)<br>
<a href="https://doi.org/10.1109/icdmw.2014.80" target="_blank">https://doi.org/10.1109/icdmw.2014.80</a>
</td>
</tr>
<tr class="error">
<td>X. U. Feng and J.-P. Zhang, “Facial microexpression recognition: A 
survey,” Acta Automatica Sinica, vol. 43, no. 3, pp. 333–348, 2017.</td>
<td>Result score too low</td>
</tr>
<tr>
<td>
<br>M. S.  ̈Ozerdem and H. Polat, “Emotion recognition based on EEG 
features in movie clips with channel selection,” Brain Inf., vol. 4, no.
 4, pp. 241–252, 2017.<br>
<a href="https://doi.org/10.1007/s40708-017-0069-3" target="_blank">https://doi.org/10.1007/s40708-017-0069-3</a>
</td>
</tr>
<tr>
<td>
<br>S. K. A. Kamarol, M. H. Jaward, H. K ̈alvi ̈ainen, J. Parkkinen, and
 R. Parthiban, “Joint facial expression recognition and intensity 
estimation based on weighted votes of image sequences,” Pattern 
Recognit. Lett., vol. 92, pp. 25–32, Jun. 2017.<br>
<a href="https://doi.org/10.1016/j.patrec.2017.04.003" target="_blank">https://doi.org/10.1016/j.patrec.2017.04.003</a>
</td>
</tr>
<tr>
<td>
<br>Hongli Zhang , Alireza Jolfaei , and Mamoun Alazab, “A Face Emotion 
Recognition Method Using Convolutional Neural Network and Image Edge 
Computing,” IEEE ACCESS, Nov, 2019. 2949741<br>
<a href="https://doi.org/10.1109/access.2019.2949741" target="_blank">https://doi.org/10.1109/access.2019.2949741</a>
</td>
</tr>
<tr>
<td>
<br>H. Ma and T. Celik, “FER-Net: Facial expression recognition using 
densely connected convolutional network,” Electron. Lett., vol. 55, no. 
4, pp. 184–186, Feb. 2019.<br>
<a href="https://doi.org/10.1049/el.2018.7871" target="_blank">https://doi.org/10.1049/el.2018.7871</a>
</td>
</tr>
<tr>
<td>
<br>A. V. Savchenko, “Deep neural networks and maximum likelihood search
 for approximate nearest neighbor in video-based image recognition,” 
Opt. Memory Neural Netw., vol. 26, no. 2, pp. 129–136, Apr. 2017.<br>
<a href="https://doi.org/10.3103/s1060992x17020102" target="_blank">https://doi.org/10.3103/s1060992x17020102</a>
</td>
</tr>
<tr>
<td>
<br>Salah, Albert Ali. Multimodal Behavior Analysis in the Wild Video-based emotion recognition in the wild. , (2019), 369-386<br>
<a href="https://doi.org/10.1016/b978-0-12-814601-9.00031-6" target="_blank">https://doi.org/10.1016/b978-0-12-814601-9.00031-6</a>
</td>
</tr>
</tbody></table>
</div>
</div>

</div>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>